{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pdb\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import * \n",
    "from model import * \n",
    "from PIL import Image\n",
    "import numpy\n",
    "from scipy.stats import truncnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function we use to produce our distributions for the gaussian dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command Line argument parsing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# data I/O\n",
    "parser.add_argument('-i', '--data_dir', type=str,\n",
    "                    default='data', help='Location for the dataset')\n",
    "parser.add_argument('-o', '--save_dir', type=str, default='models',\n",
    "                    help='Location for parameter checkpoints and samples')\n",
    "parser.add_argument('-d', '--dataset', type=str,\n",
    "                    default='cifar', help='Can be either cifar|mnist')\n",
    "parser.add_argument('-p', '--print_every', type=int, default=50,\n",
    "                    help='how many iterations between print statements')\n",
    "parser.add_argument('-t', '--save_interval', type=int, default=5,\n",
    "                    help='Every how many epochs to write checkpoint/samples?')\n",
    "parser.add_argument('-r', '--load_params', type=str, default=None,\n",
    "                    help='Restore training from previous model checkpoint?')\n",
    "# model\n",
    "parser.add_argument('-q', '--nr_resnet', type=int, default=5,\n",
    "                    help='Number of residual blocks per stage of the model')\n",
    "parser.add_argument('-n', '--nr_filters', type=int, default=160,\n",
    "                    help='Number of filters to use across the model. Higher = larger model.')\n",
    "parser.add_argument('-m', '--nr_logistic_mix', type=int, default=10,\n",
    "                    help='Number of logistic components in the mixture. Higher = more flexible model')\n",
    "parser.add_argument('-l', '--lr', type=float,\n",
    "                    default=0.0002, help='Base learning rate')\n",
    "parser.add_argument('-e', '--lr_decay', type=float, default=0.999995,\n",
    "                    help='Learning rate decay, applied every step of the optimization')\n",
    "parser.add_argument('-b', '--batch_size', type=int, default=32,\n",
    "                    help='Batch size during training per GPU')\n",
    "parser.add_argument('-x', '--max_epochs', type=int,\n",
    "                    default=5000, help='How many epochs to run in total?')\n",
    "parser.add_argument('-s', '--seed', type=int, default=1,\n",
    "                    help='Random seed to use')\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model naming, and file name creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pcnn_lr:{:.5f}_nr-resnet{}_nr-filters{}'.format(args.lr, args.nr_resnet, args.nr_filters)\n",
    "assert not os.path.exists(os.path.join('runs', model_name)), '{} already exists!'.format(model_name)\n",
    "writer = SummaryWriter(log_dir=os.path.join('runs', model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch_size = 15\n",
    "obs = (1, 28, 28) if 'mnist' in args.dataset else (3, 16, 16)\n",
    "input_channels = obs[0]\n",
    "rescaling     = lambda x : (x - .5) * 2.\n",
    "rescaling_inv = lambda x : .5 * x  + .5\n",
    "resizer=transforms.Resize((16,16))\n",
    "kwargs = {'num_workers':1, 'pin_memory':True, 'drop_last':True}\n",
    "ds_transforms = transforms.Compose([resizer,transforms.ToTensor(), rescaling])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the loss operations and datasets depending on dataset we chose in the command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if 'mnist' in args.dataset : \n",
    "    train_loader = torch.utils.data.DataLoader(datasets.MNIST(args.data_dir, download=True, \n",
    "                        train=True, transform=ds_transforms), batch_size=args.batch_size, \n",
    "                            shuffle=True, **kwargs)\n",
    "    \n",
    "    test_loader  = torch.utils.data.DataLoader(datasets.MNIST(args.data_dir, train=False, \n",
    "                    transform=ds_transforms), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    loss_op   = lambda real, fake : discretized_mix_logistic_loss_1d(real, fake)\n",
    "    sample_op = lambda x : sample_from_discretized_mix_logistic_1d(x, args.nr_logistic_mix)\n",
    "\n",
    "elif 'cifar' in args.dataset : \n",
    "    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(args.data_dir, train=True, \n",
    "        download=True, transform=ds_transforms), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    test_loader  = torch.utils.data.DataLoader(datasets.CIFAR10(args.data_dir, train=False, \n",
    "                    transform=ds_transforms), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    loss_op   = lambda real, fake : discretized_mix_logistic_loss(real, fake)\n",
    "    sample_op = lambda x : sample_from_discretized_mix_logistic(x, args.nr_logistic_mix)\n",
    "else :\n",
    "    raise Exception('{} dataset not in {mnist, cifar10}'.format(args.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates the dataset of images of constant pixel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_constant=32\n",
    "images=[torch.ones([1,3,16,16])*(-1+2*i/(len_constant-1)) for i in range(len_constant)]\n",
    "constant_images=torch.cat(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates the low-variance images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_low_variance=32\n",
    "X=get_truncated_normal(mean=0,sd=0.05,low=-1,upp=1)\n",
    "lowvar_images=torch.reshape(torch.as_tensor(X.rvs(len_low_variance*256*3),dtype=torch.float),(len_low_variance,3,16,16))\n",
    "\n",
    "model = PixelCNN(nr_resnet=args.nr_resnet, nr_filters=args.nr_filters, \n",
    "            input_channels=input_channels, nr_logistic_mix=args.nr_logistic_mix)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates the SVHN images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_svhn=torch.utils.data.DataLoader(datasets.SVHN)\n",
    "if args.load_params:\n",
    "    load_part_of_model(model, args.load_params)\n",
    "    # model.load_state_dict(torch.load(args.load_params))\n",
    "    print('model parameters loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=args.lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the method that we will use to sample from the pixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model):\n",
    "    model.train(False)\n",
    "    data = torch.zeros(sample_batch_size, obs[0], obs[1], obs[2])\n",
    "    data = data.cuda()\n",
    "    #data=torch.nn.functional.interpolate(data,scale_factor=0.5)\n",
    "    for i in range(obs[1]//2):\n",
    "        for j in range(obs[2]//2):\n",
    "            data_v = Variable(data, volatile=True)\n",
    "            out   = model(data_v, sample=True)\n",
    "            out_sample = sample_op(out)\n",
    "            data[:, :, i, j] = out_sample.data[:, :, i, j]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 5.8005, time : 65.1044 progress: 49/1562\n"
     ]
    }
   ],
   "source": [
    "print('starting training')\n",
    "writes = 0\n",
    "global_loss=0\n",
    "for epoch in range(args.max_epochs):\n",
    "    model.train(True)\n",
    "    torch.cuda.synchronize()\n",
    "    train_loss = 0.\n",
    "    time_ = time.time()\n",
    "    model.train()\n",
    "    global_loss=0\n",
    "    for batch_idx, (input,_) in enumerate(train_loader):\n",
    "        input = input.cuda()\n",
    "        #print(input)\n",
    "        #print(len(train_loader))\n",
    "        input = Variable(input)\n",
    "        output = model(input)\n",
    "        #print(output.shape)\n",
    "        loss = loss_op(input, output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data.item()\n",
    "        global_loss+=loss.data.item()\n",
    "        if (batch_idx +1) % args.print_every == 0 : \n",
    "            deno = args.print_every * args.batch_size * np.prod(obs) * np.log(2.)\n",
    "            writer.add_scalar('train/bpd', (train_loss / deno), writes)\n",
    "            print('loss : {:.4f}, time : {:.4f} progress: {}/{}'.format(\n",
    "                (train_loss / deno), \n",
    "                (time.time() - time_),batch_idx, len(train_loader)))\n",
    "            train_loss = 0.\n",
    "            writes += 1\n",
    "            time_ = time.time()\n",
    "        #if batch_idx>48:\n",
    "           # break\n",
    "    # decrease learning rate\n",
    "    \n",
    "\n",
    "    scheduler.step()\n",
    "    deno_train=batch_idx*args.batch_size*np.prod(obs)*np.log(2.) \n",
    "    torch.cuda.synchronize()\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    for batch_idx, (input,_) in enumerate(test_loader):\n",
    "        input = input.cuda()\n",
    "        #print(type(input))\n",
    "        input_var = Variable(input)\n",
    "        output = model(input_var)\n",
    "        loss = loss_op(input_var, output)\n",
    "        test_loss += loss.data.item()\n",
    "        del loss, output\n",
    "    \n",
    "    deno = batch_idx * args.batch_size * np.prod(obs) * np.log(2.)\n",
    "    writer.add_scalar('test/bpd', (test_loss / deno), writes)\n",
    "    print('test loss : %s' % (test_loss / deno))\n",
    "    print(global_loss/len(train_loader.dataset))\n",
    "    #pdb.set_trace()\n",
    "    constant_input=constant_images.cuda()\n",
    "    constant_var=Variable(constant_input)\n",
    "    output=model(constant_var)\n",
    "    loss=loss_op(constant_var,output)\n",
    "    print(\"Constant iamge likelihood: {}\".format(loss/(constant_images.shape[0]*np.prod(obs)*np.log(2.))))\n",
    "    #with open('results.csv'):\n",
    "    \n",
    "\n",
    "\n",
    "    lowvar_input=lowvar_images.cuda()\n",
    "    lowvar_var=Variable(lowvar_input)\n",
    "    output_lowvar=model(lowvar_var)\n",
    "    loss_lowvar=loss_op(lowvar_var,output_lowvar)\n",
    "    print(\"Low var image likelihood: {}\".format(loss_lowvar/(lowvar_images.shape[0]*np.prod(obs)*np.log(2.))))\n",
    "    with open('results.csv','a') as f:\n",
    "        f.write(\"{},{},{},{},{}\".format(epoch,global_loss/deno_train,test_loss/deno, loss/(constant_images.shape[0]*np.prod(obs)*np.log(2,)),loss_lowvar/(lowvar_images.shape[0]*np.prod(obs)*np.log(2,))))\n",
    "    \n",
    "    if (epoch + 1) % args.save_interval == 0: \n",
    "        torch.save(model.state_dict(), 'models/{}_{}.pth'.format(model_name, epoch))\n",
    "        #print('sampling...')\n",
    "        #sample_t = sample(model)\n",
    "        #sample_t = rescaling_inv(sample_t)\n",
    "        #utils.save_image(sample_t,'images/{}_{}.png'.format(model_name, epoch), \n",
    "        #       nrow=5, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
